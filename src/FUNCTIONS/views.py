from pyspark.sql import SparkSession
from pyspark.sql.types import *

spark = SparkSession.builder \
    .appName("DataViewingPractice") \
    .getOrCreate()

data = [
    (1, "Manasa", 50000.0, 25),
    (2, "Mevin", 60000.0, 28),
    (3, "Pavithra", 55000.0, 26),
    (4, "Menakha", 70000.0, 30),
    (5, "Arjun", 45000.0, 24)
]

schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("name", StringType(), True),
    StructField("salary", DoubleType(), True),
    StructField("age", IntegerType(), True)
])

df = spark.createDataFrame(data, schema)

# Default 20 rows (truncated) â†’ | id | name | salary |
df.show(10)  
# First 10 rows â†’ table format

df.show(5, False)  
# No truncation â†’ full string visible

df.show(5, True, True)  
# Vertical view â†’ -RECORD 0- id:1 name:Manasa salary:50000

df.show(truncate=30)
# String cut at 30 characters

df.show(vertical=True)  
# One record per block (good for wide tables)

# SCHEMA & STRUCTURE


df.printSchema()
# Tree format â†’ root |-- id:int |-- name:string

# we have toprint upcoming three to see output

df.schema
# StructType([StructField('id', IntegerType(), True)])
df.columns
# ['id', 'name', 'salary']
df.dtypes
# [('id','int'), ('name','string')]

# ðŸ”¹ ROW ACCESS


df.first()
# First row â†’ Row(id=1, name='Manasa')
df.head()
# Same as first() â†’ Row(...)
df.head(3)
# List of 3 rows â†’ [Row(...), Row(...)]
df.take(3)
# List of first 3 rows â†’ [Row(...)]
df.collect()
# ALL rows to driver â†’ [Row(...), Row(...)]

# ðŸ”¹ COUNT

df.count()
# Total row count â†’ 1000


# ðŸ”¹ STATISTICS
df.describe().show()
# count, mean, stddev, min, max